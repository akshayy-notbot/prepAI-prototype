#!/usr/bin/env python3
"""
Enhanced startup script for Render deployment
This runs automatically when the service starts and ensures the database is properly configured

NOTE: In production, environment variables are managed through Render's dashboard.
      This script validates the configuration provided by Render.
"""

import os
import sys
import time
import json
from pathlib import Path
from sqlalchemy import text

# Add the current directory to Python path
sys.path.append(str(Path(__file__).parent))

def run_startup_checks():
    """Run all necessary startup checks and migrations"""
    
    print("üöÄ PrepAI Enhanced Startup Script - Render Deployment")
    print("=" * 60)
    print("Environment variables are managed through Render's dashboard")
    print()
    
    # Check environment variables
    print("üîç Checking environment variables...")
    required_vars = ['DATABASE_URL', 'GOOGLE_API_KEY', 'REDIS_URL']
    
    env_status = {}
    for var in required_vars:
        value = os.getenv(var)
        if not value:
            print(f"‚ùå Warning: {var} not set")
            env_status[var] = False
        else:
            # Show first 10 characters for debugging (safe for API keys)
            display_value = value[:10] + "..." if len(value) > 10 else value
            print(f"‚úÖ {var} is configured: {display_value}")
            env_status[var] = True
    
    # Validate environment variable formats
    print("\nüîç Validating environment variable formats...")
    if env_status.get('DATABASE_URL'):
        db_url = os.getenv('DATABASE_URL')
        if not db_url.startswith('postgresql://'):
            print("‚ùå DATABASE_URL must start with 'postgresql://'")
            env_status['DATABASE_URL'] = False
        else:
            print("‚úÖ DATABASE_URL format is valid")
    
    if env_status.get('REDIS_URL'):
        redis_url = os.getenv('REDIS_URL')
        if not redis_url.startswith('redis://'):
            print("‚ùå REDIS_URL must start with 'redis://'")
            env_status['REDIS_URL'] = False
        else:
            print("‚úÖ REDIS_URL format is valid")
    
    if env_status.get('GOOGLE_API_KEY'):
        api_key = os.getenv('GOOGLE_API_KEY')
        if 'your_' in api_key or 'placeholder' in api_key:
            print("‚ùå GOOGLE_API_KEY contains placeholder value")
            env_status['GOOGLE_API_KEY'] = False
        else:
            print("‚úÖ GOOGLE_API_KEY appears to be valid")
    
    # Wait for database to be ready (important for Render)
    print("\n‚è≥ Waiting for database connection...")
    max_retries = 30
    retry_count = 0
    
    while retry_count < max_retries:
        try:
            from models import get_engine, Base
            # Test connection using SQLAlchemy 2.0+ syntax
            with get_engine().connect() as conn:
                result = conn.execute(text("SELECT 1"))
                result.fetchone()  # Consume the result
            print("‚úÖ Database connection successful")
            break
        except Exception as e:
            retry_count += 1
            print(f"‚è≥ Database not ready (attempt {retry_count}/{max_retries}): {e}")
            if retry_count < max_retries:
                time.sleep(2)
            else:
                print("‚ùå Failed to connect to database after maximum retries")
                return False
    
    # Create database schema
    print("\nüìã Creating database schema...")
    try:
        Base.metadata.create_all(bind=get_engine())
        print("‚úÖ Database schema created/updated successfully")
    except Exception as e:
        print(f"‚ùå Failed to create database schema: {e}")
        return False
    
    # Create interview_playbooks table if it doesn't exist
    print("\nüìã Creating interview_playbooks table...")
    try:
        from sqlalchemy import inspect
        inspector = inspect(get_engine())
        existing_tables = inspector.get_table_names()
        
        if 'interview_playbooks' not in existing_tables:
            print("üîÑ Creating interview_playbooks table...")
            with get_engine().connect() as connection:
                trans = connection.begin()
                try:
                    # Create the interview_playbooks table
                    connection.execute(text("""
                        CREATE TABLE interview_playbooks (
                            id SERIAL PRIMARY KEY,
                            role VARCHAR(255) NOT NULL,
                            skill VARCHAR(255) NOT NULL,
                            seniority VARCHAR(255) NOT NULL,
                            archetype VARCHAR(255),
                            interview_objective TEXT,
                            evaluation_dimensions JSONB,
                            seniority_criteria JSONB,
                            good_vs_great_examples JSONB,
                            pre_interview_strategy TEXT,
                            during_interview_execution TEXT,
                            post_interview_evaluation TEXT,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                        )
                    """))
                    
                    # Create indexes for better performance
                    connection.execute(text("CREATE INDEX IF NOT EXISTS idx_interview_playbooks_role ON interview_playbooks(role)"))
                    connection.execute(text("CREATE INDEX IF NOT EXISTS idx_interview_playbooks_skill ON interview_playbooks(skill)"))
                    connection.execute(text("CREATE INDEX IF NOT EXISTS idx_interview_playbooks_seniority ON interview_playbooks(seniority)"))
                    connection.execute(text("CREATE INDEX IF NOT EXISTS idx_interview_playbooks_combo ON interview_playbooks(role, skill, seniority)"))
                    
                    trans.commit()
                    print("‚úÖ interview_playbooks table created successfully with indexes")
                except Exception as e:
                    trans.rollback()
                    print(f"‚ùå Failed to create interview_playbooks table: {e}")
                    raise
        else:
            print("‚úÖ interview_playbooks table already exists")
            
        # Check if core_philosophy column exists and add it if missing
        if 'interview_playbooks' in existing_tables:
            print("üîç Checking for core_philosophy column...")
            columns = [col['name'] for col in inspector.get_columns('interview_playbooks')]
            
            if 'core_philosophy' not in columns:
                print("üîÑ Adding core_philosophy column to interview_playbooks table...")
                with get_engine().connect() as connection:
                    trans = connection.begin()
                    try:
                        connection.execute(text("""
                            ALTER TABLE interview_playbooks 
                            ADD COLUMN core_philosophy TEXT
                        """))
                        trans.commit()
                        print("‚úÖ core_philosophy column added successfully")
                    except Exception as e:
                        trans.rollback()
                        print(f"‚ùå Failed to add core_philosophy column: {e}")
                        raise
            else:
                print("‚úÖ core_philosophy column already exists")
            
    except Exception as e:
        print(f"‚ùå Error creating interview_playbooks table: {e}")
        return False
    
    # Run specific migrations
    print("\nüîÑ Running database migrations...")
    migration_status = {}
    
    try:
        from sqlalchemy import inspect
        inspector = inspect(get_engine())
        
        # Check interview_sessions table for playbook_id column
        if 'interview_sessions' in inspector.get_table_names():
            session_columns = [col['name'] for col in inspector.get_columns('interview_sessions')]
            print(f"üìã Current interview_sessions columns: {session_columns}")
            
            # Migration 0: Add all missing columns to interview_sessions
            required_session_columns = {
                'role': 'VARCHAR(255)',
                'seniority': 'VARCHAR(255)',
                'skill': 'VARCHAR(255)',
                'playbook_id': 'INTEGER',
                'selected_archetype': 'VARCHAR(255)',
                'generated_prompt': 'TEXT',
                'signal_map': 'JSON',
                'evaluation_criteria': 'JSON',
                'conversation_history': 'JSON',
                'collected_signals': 'JSON',
                'final_evaluation': 'JSON',
                'interview_started_at': 'TIMESTAMP',
                'interview_completed_at': 'TIMESTAMP'
            }
            
            for column_name, column_type in required_session_columns.items():
                if column_name not in session_columns:
                    print(f"üîÑ Adding {column_name} column to interview_sessions table...")
                    try:
                        with get_engine().connect() as connection:
                            trans = connection.begin()
                            try:
                                if column_name == 'playbook_id':
                                    # Add foreign key constraint for playbook_id
                                    connection.execute(text(f"""
                                        ALTER TABLE interview_sessions 
                                        ADD COLUMN {column_name} {column_type}
                                    """))
                                else:
                                    # Add other columns
                                    connection.execute(text(f"""
                                        ALTER TABLE interview_sessions 
                                        ADD COLUMN {column_name} {column_type}
                                    """))
                                trans.commit()
                                print(f"‚úÖ {column_name} column added successfully")
                                migration_status[column_name] = 'ADDED'
                            except Exception as e:
                                trans.rollback()
                                print(f"‚ùå Failed to add {column_name} column: {e}")
                                raise
                    except Exception as e:
                        print(f"‚ùå Error adding {column_name} column: {e}")
                        return False
                else:
                    print(f"‚úÖ {column_name} column already exists in interview_sessions")
                    migration_status[column_name] = 'EXISTS'
        
        # Check session_states table
        columns = [col['name'] for col in inspector.get_columns('session_states')]
        print(f"üìã Current session_states columns: {columns}")
        
        # Migration 1: Add complete_interview_data column
        if 'complete_interview_data' not in columns:
            print("üîÑ Adding complete_interview_data column to session_states table...")
            try:
                with get_engine().connect() as connection:
                    # Use explicit transaction to ensure the column is added
                    trans = connection.begin()
                    try:
                        connection.execute(text("""
                            ALTER TABLE session_states 
                            ADD COLUMN complete_interview_data JSON
                        """))
                        trans.commit()
                        print("‚úÖ complete_interview_data column added successfully")
                        migration_status['complete_interview_data'] = 'ADDED'
                    except Exception as e:
                        trans.rollback()
                        print(f"‚ùå Failed to add complete_interview_data column: {e}")
                        raise
            except Exception as e:
                print(f"‚ùå Error adding complete_interview_data column: {e}")
                return False
        else:
            print("‚úÖ complete_interview_data column already exists")
            migration_status['complete_interview_data'] = 'EXISTS'
        
        # Migration 2: Add average_score column
        if 'average_score' not in columns:
            print("üîÑ Adding average_score column to session_states table...")
            try:
                with get_engine().connect() as connection:
                    # Use explicit transaction to ensure the column is added
                    trans = connection.begin()
                    try:
                        connection.execute(text("""
                            ALTER TABLE session_states 
                            ADD COLUMN average_score INTEGER
                        """))
                        trans.commit()
                        print("‚úÖ average_score column added successfully")
                        migration_status['average_score'] = 'ADDED'
                    except Exception as e:
                        trans.rollback()
                        print(f"‚ùå Failed to add average_score column: {e}")
                        raise
            except Exception as e:
                print(f"‚ùå Error adding average_score column: {e}")
                return False
        else:
            print("‚úÖ average_score column already exists")
            migration_status['average_score'] = 'EXISTS'
        
        # Wait a moment for the database to reflect changes
        print("‚è≥ Waiting for database changes to propagate...")
        time.sleep(2)
        
        # Direct SQL verification of column addition
        print("\nüîç Direct SQL verification of column addition...")
        try:
            with get_engine().connect() as connection:
                # Check current schema
                schema_result = connection.execute(text("SELECT current_schema()"))
                current_schema = schema_result.fetchone()[0]
                print(f"üìã Current database schema: {current_schema}")
                
                # Check if columns exist using direct SQL
                result = connection.execute(text("""
                    SELECT column_name, data_type, table_schema
                    FROM information_schema.columns 
                    WHERE table_name = 'session_states' 
                    AND column_name IN ('complete_interview_data', 'average_score')
                    ORDER BY column_name
                """))
                
                sql_columns = result.fetchall()
                print(f"üìã SQL verification found columns: {sql_columns}")
                
                if len(sql_columns) == 2:
                    print("‚úÖ Both columns confirmed via direct SQL query")
                else:
                    print(f"‚ö†Ô∏è  Only {len(sql_columns)} columns found via SQL")
                    
                    # Check all columns in the table to see what's there
                    all_columns_result = connection.execute(text("""
                        SELECT column_name, data_type, table_schema
                        FROM information_schema.columns 
                        WHERE table_name = 'session_states'
                        ORDER BY column_name
                    """))
                    
                    all_columns = all_columns_result.fetchall()
                    print(f"üìã All columns in session_states table: {all_columns}")
                    
        except Exception as e:
            print(f"‚ö†Ô∏è  SQL verification failed: {e}")
        
        # Verify final schema with fresh inspection
        print("\nüîç Verifying final database schema...")
        try:
            # Refresh the inspector to get the latest schema
            inspector = inspect(get_engine())
            final_columns = [col['name'] for col in inspector.get_columns('session_states')]
            print(f"üìã Final table columns: {final_columns}")
            
            # Check migration success for session_states
            required_columns = ['complete_interview_data', 'average_score']
            all_migrations_successful = all(col in final_columns for col in required_columns)
            
            # Check interview_sessions table for all required columns
            if 'interview_sessions' in inspector.get_table_names():
                session_columns = [col['name'] for col in inspector.get_columns('interview_sessions')]
                required_session_columns = [
                    'role', 'seniority', 'skill', 'playbook_id', 'selected_archetype', 
                    'generated_prompt', 'signal_map', 'evaluation_criteria', 
                    'conversation_history', 'collected_signals', 'final_evaluation', 
                    'interview_started_at', 'interview_completed_at'
                ]
                
                missing_session_columns = [col for col in required_session_columns if col not in session_columns]
                if missing_session_columns:
                    print(f"‚ùå Missing columns from interview_sessions table: {missing_session_columns}")
                    all_migrations_successful = False
                else:
                    print("‚úÖ All required columns verified in interview_sessions table")
            
            if all_migrations_successful:
                print("üéâ All database migrations completed successfully!")
                print("üìä Migration Summary:")
                for col, status in migration_status.items():
                    print(f"   ‚Ä¢ {col}: {status}")
                
                # Migration verification summary
                print("\nüîç Migration verification summary...")
                print("‚úÖ Database columns added successfully")
                print("‚úÖ JSON column type verified via schema inspection")
                print("‚úÖ Integer column type verified via schema inspection")
                print("‚úÖ All required migrations completed")
                    
            else:
                print("‚ùå Some required columns are missing after migration")
                missing = [col for col in required_columns if col not in final_columns]
                print(f"   Missing columns: {missing}")
                
                # Additional debugging information
                print("\nüîç Debugging migration issue...")
                print("üìã Expected columns after migration:")
                for col in required_columns:
                    if col in final_columns:
                        print(f"   ‚úÖ {col}: EXISTS")
                    else:
                        print(f"   ‚ùå {col}: MISSING")
                
                # Check if columns exist with different names
                print("\nüîç Checking for similar column names...")
                for col in final_columns:
                    if 'complete' in col.lower() or 'interview' in col.lower() or 'data' in col.lower():
                        print(f"   üîç Similar column found: {col}")
                    if 'score' in col.lower() or 'average' in col.lower():
                        print(f"   üîç Similar column found: {col}")
                
                return False
                
        except Exception as e:
            print(f"‚ùå Error during final schema verification: {e}")
            return False
        
    except Exception as e:
        print(f"‚ùå Database migrations failed: {e}")
        return False
    
    # Test Redis connection
    print("\nüîç Testing Redis connection...")
    try:
        import redis
        redis_url = os.getenv('REDIS_URL')
        if redis_url:
            redis_client = redis.from_url(redis_url, decode_responses=True)
            redis_client.ping()
            print("‚úÖ Redis connection successful")
            
            # Test critical Redis operations for new architecture
            print("üîç Testing Redis JSON operations...")
            test_key = "startup_test"
            test_data = {"test": "data", "timestamp": time.time()}
            
            import json
            redis_client.set(test_key, json.dumps(test_data), ex=60)
            retrieved = redis_client.get(test_key)
            
            if retrieved and json.loads(retrieved) == test_data:
                print("‚úÖ Redis JSON operations working")
            else:
                print("‚ùå Redis JSON operations failed")
                return False
            
            # Cleanup test data
            redis_client.delete(test_key)
            print("‚úÖ Redis test cleanup successful")
            
        else:
            print("‚ö†Ô∏è  REDIS_URL not set")
    except Exception as e:
        print(f"‚ùå Redis connection failed: {e}")
        print("‚ö†Ô∏è  Some features may not work without Redis")
        if not env_status.get('REDIS_URL'):
            print("‚ùå Redis is required for the new architecture")
            return False
    
    # Verify tables exist
    print("\nüîç Verifying database tables...")
    try:
        from models import get_session_local, SessionState, InterviewPlaybook
        
        db = get_session_local()()
        
        # Check if session_states table exists and has data
        try:
            count = db.query(SessionState).count()
            print(f"‚úÖ session_states table: {count} records")
        except Exception as e:
            print(f"‚ùå session_states table: {e}")
        
        # Check if interview_playbooks table exists and has data
        try:
            playbook_count = db.query(InterviewPlaybook).count()
            print(f"‚úÖ interview_playbooks table: {playbook_count} records")
            if playbook_count == 0:
                print("‚ÑπÔ∏è  interview_playbooks table is empty - ready for data import")
        except Exception as e:
            print(f"‚ùå interview_playbooks table: {e}")
        
        db.close()
        
    except Exception as e:
        print(f"‚ùå Failed to verify tables: {e}")
        return False
    
    # Test autonomous interviewer components
    print("\nüèóÔ∏è Testing autonomous interviewer components...")
    try:
        from agents.autonomous_interviewer import AutonomousInterviewer
        from agents.session_tracker import SessionTracker
        
        # Test component initialization
        autonomous_interviewer = AutonomousInterviewer()
        session_tracker = SessionTracker()
        print("‚úÖ Autonomous interviewer components initialized successfully")
        
        # Test session tracker operations
        test_session_id = "startup_test_session"
        test_session_data = {
            "role": "Software Engineer",
            "seniority": "Senior",
            "skill": "System Design"
        }
        
        # Test session creation
        session_data = session_tracker.create_session(
            session_id=test_session_id,
            role=test_session_data["role"],
            seniority=test_session_data["seniority"],
            skill=test_session_data["skill"]
        )
        
        if session_data and session_data.get("role") == "Software Engineer":
            print("‚úÖ Session tracker operations working")
        else:
            print("‚ùå Session tracker operations failed")
            return False
        
        # Cleanup test session
        session_tracker.delete_session(test_session_id)
        print("‚úÖ Architecture test cleanup successful")
        
    except Exception as e:
        print(f"‚ùå Architecture component test failed: {e}")
        return False
    
    # Final validation summary
    print("\nüìä Final Validation Summary")
    print("=" * 40)
    
    critical_checks = [
        ("Environment Variables", all(env_status.values())),
        ("Database Connection", True),  # Already verified above
        ("Database Schema", True),      # Already verified above
        ("Database Migrations", True),  # Already verified above
        ("Redis Connection", env_status.get('REDIS_URL', False)),
        ("Architecture Components", True)  # Already verified above
    ]
    
    for check_name, status in critical_checks:
        status_icon = "‚úÖ" if status else "‚ùå"
        print(f"{status_icon} {check_name}")
    
    all_passed = all(status for _, status in critical_checks)
    
    if all_passed:
        print("\nüéâ All critical checks passed! PrepAI is ready to serve requests!")
        print("\nüöÄ Deployment Status: READY")
        print("üìã Next Steps:")
        print("   1. Monitor application logs for any runtime issues")
        print("   2. Test the interview flow with a sample session")
        print("   3. Verify Redis session management is working")
        print("   4. Check database performance under load")
        print("\nüóÑÔ∏è  Database Migration Status:")
        print("   ‚úÖ interview_sessions table: All required columns added (playbook_id, selected_archetype, generated_prompt, etc.)")
        print("   ‚úÖ complete_interview_data column: JSON support for enhanced data storage")
        print("   ‚úÖ average_score column: INTEGER support for performance tracking")
        print("   ‚úÖ All migrations completed and verified via schema inspection")
        print("   ‚úÖ Database schema matches deployed requirements")
        print("\nüîê Environment Variables:")
        print("   ‚Ä¢ DATABASE_URL: Set by Render PostgreSQL service")
        print("   ‚Ä¢ REDIS_URL: Set by Render Redis service")
        print("   ‚Ä¢ GOOGLE_API_KEY: Set in Render environment variables")
        print("   ‚Ä¢ ENVIRONMENT: Set to 'production' in Render")
    else:
        print("\n‚ö†Ô∏è  Some critical checks failed. Service may not work properly.")
        print("üìã Failed Checks:")
        for check_name, status in critical_checks:
            if not status:
                print(f"   ‚Ä¢ {check_name}")
        print("\nüîß Troubleshooting:")
        print("   ‚Ä¢ Check Render dashboard for environment variable configuration")
        print("   ‚Ä¢ Verify PostgreSQL and Redis services are running")
        print("   ‚Ä¢ Check service logs for detailed error messages")
    
    return all_passed

if __name__ == "__main__":
    success = run_startup_checks()
    if not success:
        print("\n‚ùå Startup failed - service may not work properly")
        sys.exit(1)
    else:
        print("\n‚úÖ PrepAI is ready to serve requests!")
