import os
import json
import time
import random
from typing import List, Dict, Any
from datetime import datetime

# Load environment variables from .env file if it exists
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    print("âš ï¸  dotenv not available, using system environment variables")

try:
    from fastapi import FastAPI, Depends, HTTPException
    from fastapi.middleware.cors import CORSMiddleware
    from pydantic import BaseModel
    from sqlalchemy.orm import Session
    print("âœ… FastAPI components imported successfully")
except Exception as e:
    print(f"âŒ Failed to import FastAPI components: {e}")
    raise

try:
    import redis
    print("âœ… redis imported successfully")
except Exception as e:
    print(f"âŒ Failed to import redis: {e}")
    raise

# Import our autonomous interviewer components
try:
    from agents.autonomous_interviewer import AutonomousInterviewer
    from agents.session_tracker import SessionTracker
    from agents.evaluation import evaluate_answer
    from agents.pre_interview_planner import PreInterviewPlanner
    from agents.interview_evaluator import InterviewEvaluator
    from models import persist_complete_interview, persist_interview_session
    print("âœ… Autonomous interviewer components imported successfully")
except Exception as e:
    print(f"âŒ Failed to import autonomous interviewer components: {e}")
    raise

# --- FastAPI App Initialization ---
app = FastAPI(title="PrepAI Autonomous Interviewer API")

# --- CORS Middleware Configuration ---
origins = [
    "https://akshayy-notbot.github.io",
    "https://prepai-api.onrender.com",
    "http://127.0.0.1:5500",
    "http://localhost:8000",
    "http://127.0.0.1:8000",
    "null",
    "*",
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Pydantic Models ---
class StartInterviewRequest(BaseModel):
    role: str
    seniority: str
    skills: List[str]

class SubmitAnswerRequest(BaseModel):
    session_id: str
    answer: str

class EvaluateInterviewRequest(BaseModel):
    role: str
    seniority: str
    skills: List[str]
    transcript: List[Dict[str, Any]]

# --- Root Endpoint ---
@app.get("/")
@app.head("/")
async def root():
    """Root endpoint for health checks and service discovery"""
    return {
        "message": "PrepAI Autonomous Interviewer API",
        "status": "running",
        "health_check": "/health",
        "docs": "/docs"
    }

# --- Evaluation Endpoint ---
@app.post("/api/evaluate-interview")
async def evaluate_interview(request: EvaluateInterviewRequest):
    """
    Evaluate an interview transcript and generate feedback.
    """
    try:
        print(f"ðŸ” Evaluating interview for {request.role} role ({request.seniority} level)")
        
        # Extract Q&A pairs from transcript
        qa_pairs = []
        for item in request.transcript:
            if item.get("question") and item.get("answer"):
                qa_pairs.append({
                    "question": item["question"],
                    "answer": item["answer"]
                })
        
        if not qa_pairs:
            raise HTTPException(status_code=400, detail="No valid question-answer pairs found in transcript")
        
        print(f"ðŸ“Š Found {len(qa_pairs)} Q&A pairs for evaluation")
        
        # Evaluate each answer using the evaluation agent
        evaluations = []
        skills_to_assess = request.skills if request.skills else ["Problem Solving", "Communication", "Technical Knowledge"]
        
        for i, qa in enumerate(qa_pairs):
            print(f"ðŸ” Evaluating Q: {qa['question'][:50]}...")
            
            # Get conversation history up to this point for context
            conversation_history = qa_pairs[:i] if i > 0 else []
            
            # Create role context for better evaluation
            role_context = {
                "role": request.role,
                "seniority": request.seniority
            }
            
            evaluation = evaluate_answer(qa["answer"], qa["question"], skills_to_assess, conversation_history, role_context)
            evaluations.append({
                "question": qa["question"],
                "answer": qa["answer"],
                "evaluation": evaluation
            })
        
        # Calculate overall scores and extract feedback
        all_scores = []
        strengths = []
        improvements = []
        category_feedback = {}
        combined_scores = {}  # Aggregate scores across all questions
        
        for eval_data in evaluations:
            if "evaluation" in eval_data and "scores" in eval_data["evaluation"]:
                scores = eval_data["evaluation"]["scores"]
                
                # Aggregate scores across all questions for each skill
                for skill, score_data in scores.items():
                    if isinstance(score_data, dict) and "score" in score_data:
                        score = score_data["score"]
                        all_scores.append(score)
                        
                        # Initialize skill tracking if not exists
                        if skill not in combined_scores:
                            combined_scores[skill] = {
                                "scores": [],
                                "feedback": []
                            }
                        
                        # Collect all scores and feedback for this skill
                        combined_scores[skill]["scores"].append(score)
                        if "feedback" in score_data:
                            combined_scores[skill]["feedback"].append(score_data["feedback"])
                        
                        # Categorize feedback for strengths/improvements
                        feedback_text = score_data.get("feedback", "")
                        if score >= 4:
                            strengths.append(f"{skill}: {feedback_text}")
                        elif score <= 2:
                            improvements.append(f"{skill}: {feedback_text}")
                        
                        category_feedback[skill] = feedback_text
        
        # Calculate average scores for each skill
        skill_scores = {}
        for skill, data in combined_scores.items():
            if data["scores"]:
                avg_score = sum(data["scores"]) / len(data["scores"])
                # Get the most recent feedback for this skill
                latest_feedback = data["feedback"][-1] if data["feedback"] else f"Average performance in {skill}"
                
                skill_scores[skill] = {
                    "score": round(avg_score, 1),
                    "feedback": latest_feedback
                }
        
        # Calculate overall score on 0-5 scale (keep original scale)
        overall_score_5_scale = round(sum(all_scores) / len(all_scores), 1) if all_scores else 0
        
        # Generate response matching feedback page expectations
        feedback_response = {
            "overall_score": overall_score_5_scale,  # Use 0-5 scale for frontend
            "overall_feedback": f"Your interview performance shows {'strong' if overall_score_5_scale >= 4.0 else 'good' if overall_score_5_scale >= 3.0 else 'developing'} skills in {request.role} for {request.seniority} level.",
            "scores": skill_scores,  # Individual skill scores for the UI
            "strengths": strengths[:3] if strengths else ["Good communication throughout the interview"],
            "improvements": improvements[:3] if improvements else ["Continue practicing to build confidence"],
            "category_feedback": category_feedback,
            "recommendations": [
                f"Focus on practicing {request.skills[0] if request.skills else 'core skills'} with real-world examples",
                "Review industry best practices and current trends",
                "Practice articulating your thought process clearly"
            ],
            "questions_evaluated": len(qa_pairs),
            "role": request.role,
            "seniority": request.seniority,
            "skills_assessed": skills_to_assess,
            "next_steps": [
                "Practice similar questions to build confidence",
                "Review your answers and identify areas for improvement",
                "Consider mock interviews with different scenarios"
            ],
            "detailed_evaluations": evaluations  # Include full evaluations with ideal responses
        }
        
        print(f"âœ… Evaluation completed successfully with overall score: {overall_score_5_scale}/5")
        print(f"ðŸ“Š Skill breakdown: {skill_scores}")
        return feedback_response
        
    except Exception as e:
        print(f"âŒ Error in evaluate_interview: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to evaluate interview: {str(e)}")

# --- Enhanced Interview Evaluation Endpoint ---
@app.post("/api/evaluate-interview-enhanced")
async def evaluate_interview_enhanced(request: EvaluateInterviewRequest):
    """
    Enhanced interview evaluation using the new InterviewEvaluator.
    Provides comprehensive dimension-by-dimension assessment with signal evidence.
    """
    try:
        print(f"ðŸ” Enhanced evaluation for {request.role} role ({request.seniority} level)")
        
        # Get the interview plan from the session context (if available)
        # For now, we'll create a basic plan structure
        interview_plan = {
            "top_evaluation_dimensions": request.skills if request.skills else ["Problem Solving", "Communication", "Technical Knowledge"],
            "selected_archetype": "comprehensive",
            "interview_objective": f"Assess {request.role} capabilities at {request.seniority} level",
            "seniority_criteria": {
                "junior": "Basic understanding and application",
                "mid": "Structured approach with some depth",
                "senior": "Strategic thinking and comprehensive analysis",
                "staff+": "Innovative approaches and thought leadership"
            },
            "good_vs_great_examples": {
                "good": "Competent, covers basics, logical approach",
                "great": "Insightful, innovative, considers edge cases, shows deep understanding"
            }
        }
        
        # Convert transcript to conversation history format
        conversation_history = []
        for item in request.transcript:
            if item.get("question"):
                conversation_history.append({
                    "role": "interviewer",
                    "content": item["question"],
                    "timestamp": item.get("timestamp", "")
                })
            if item.get("answer"):
                conversation_history.append({
                    "role": "candidate",
                    "content": item["answer"],
                    "timestamp": item.get("timestamp", "")
                })
        
        # Create signal evidence from the transcript
        signal_evidence = {}
        for skill in interview_plan["top_evaluation_dimensions"]:
            signal_evidence[skill] = {
                "positive_signals": [],
                "areas_for_improvement": [],
                "quotes": [],
                "confidence": "Medium"
            }
            
            # Simple signal extraction (in production, this would be more sophisticated)
            for item in request.transcript:
                if item.get("answer"):
                    answer = item["answer"].lower()
                    if any(word in answer for word in ["think", "approach", "strategy"]):
                        signal_evidence[skill]["positive_signals"].append("Shows strategic thinking")
                    if any(word in answer for word in ["user", "customer", "need"]):
                        signal_evidence[skill]["positive_signals"].append("Demonstrates user empathy")
                    signal_evidence[skill]["quotes"].append(item["answer"][:100] + "...")
        
        # Use the InterviewEvaluator for comprehensive evaluation
        evaluator = InterviewEvaluator()
        evaluation_result = evaluator.evaluate_interview(
            role=request.role,
            seniority=request.seniority,
            skill=request.skills[0] if request.skills else "General",
            conversation_history=conversation_history,
            signal_evidence=signal_evidence,
            interview_plan=interview_plan
        )
        
        if "error" in evaluation_result:
            raise Exception(evaluation_result["error"])
        
        # Generate human-readable summary
        summary = evaluator.generate_evaluation_summary(evaluation_result)
        
        # Return comprehensive evaluation
        return {
            "success": True,
            "evaluation": evaluation_result,
            "summary": summary,
            "metadata": evaluation_result.get("evaluation_metadata", {}),
            "message": "Enhanced evaluation completed successfully"
        }
        
    except Exception as e:
        print(f"âŒ Error in enhanced evaluation: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to perform enhanced evaluation: {str(e)}")

# --- Health Check Endpoint ---
@app.get("/health")
async def health_check():
    """Health check endpoint for Render deployment"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "service": "PrepAI Autonomous Interviewer Backend"
    }

# --- Interview Endpoints ---
@app.post("/api/start-interview")
async def start_interview(request: StartInterviewRequest):
    """
    Starts a new autonomous interview session.
    Creates a session and generates the first question.
    """
    try:
        print(f"ðŸš€ Starting interview for {request.role} at {request.seniority} level")
        print(f"ðŸ“š Skills to practice: {', '.join(request.skills)}")
        
        # Generate unique session ID
        session_id = f"session_{int(time.time())}_{random.randint(1000, 9999)}"
        print(f"âœ… Generated session ID: {session_id}")
        
        # Step 1: Create interview plan using PreInterviewPlanner
        print("ðŸ“‹ Creating interview plan...")
        try:
            planner = PreInterviewPlanner()
            interview_plan = planner.create_interview_plan(
                role=request.role,
                skill=request.skills[0] if request.skills else "General",
                seniority=request.seniority
            )
            print(f"âœ… Interview plan created with archetype: {interview_plan['selected_archetype']}")
            
        except Exception as planning_error:
            error_msg = str(planning_error)
            print(f"âŒ Failed to create interview plan: {error_msg}")
            
            # Provide helpful error messages based on the error
            if "No interview playbook found" in error_msg:
                return {
                    "error": "Interview playbook not found",
                    "message": f"No interview playbook exists for {request.role} - {request.skills[0] if request.skills else 'General'} - {request.seniority}. Please ensure the playbook exists in the database.",
                    "status_code": 404
                }, 404
            elif "No evaluation dimensions found" in error_msg:
                return {
                    "error": "Incomplete playbook data",
                    "message": f"The interview playbook for {request.role} - {request.skills[0] if request.skills else 'General'} - {request.seniority} is missing evaluation dimensions. Please update the playbook.",
                    "status_code": 400
                }, 400
            elif "No interview objective found" in error_msg:
                return {
                    "error": "Incomplete playbook data",
                    "message": f"The interview playbook for {request.role} - {request.skills[0] if request.skills else 'General'} - {request.seniority} is missing interview objective. Please update the playbook.",
                    "status_code": 400
                }, 400
            else:
                return {
                    "error": "Interview planning failed",
                    "message": f"Failed to create interview plan: {error_msg}",
                    "status_code": 500
                }, 500
        
        # Step 2: Create interview session in database
        print("ðŸ’¾ Creating interview session...")
        try:
            session_data = {
                "session_id": session_id,
                "selected_archetype": interview_plan["selected_archetype"],
                "generated_prompt": interview_plan["interview_prompt"],
                "conversation_history": [],
                "collected_signals": {},
                "final_evaluation": None
            }
            
            if not persist_interview_session(session_data):
                raise Exception("Failed to persist interview session to database")
            
            print(f"âœ… Interview session created and persisted")
            
        except Exception as session_error:
            print(f"âŒ Failed to create session: {session_error}")
            return {"error": f"Failed to create interview session: {session_error}"}, 500
        
        # Step 3: Initialize Session Tracker and Autonomous Interviewer
        try:
            session_tracker = SessionTracker()
            autonomous_interviewer = AutonomousInterviewer()
            
            # Create new session with simplified structure
            session_tracker.create_session(
                session_id=session_id,
                role=request.role,
                seniority=request.seniority,
                skill=request.skills[0] if request.skills else "General"
            )
            
            print(f"âœ… Session tracker initialized successfully")
            
        except Exception as tracker_error:
            print(f"âŒ Failed to initialize session tracker: {tracker_error}")
            # Continue anyway since we have the main session in database
        
        # Step 4: Generate the First Question using the interview plan
        print("ðŸŽ­ Generating first interview question using interview plan...")
        
        try:
            # Get initial question from autonomous interviewer using the plan
            first_question_result = autonomous_interviewer.get_initial_question(
                role=request.role,
                seniority=request.seniority,
                skill=request.skills[0] if request.skills else "General",
                session_context={"interview_plan": interview_plan},
                interview_plan=interview_plan
            )
            
            if not first_question_result.get("response_text"):
                raise Exception("Autonomous Interviewer failed to generate opening statement")
            
            opening_statement = first_question_result["response_text"]
            print(f"âœ… Opening statement generated: {opening_statement[:100]}...")
            
            # Update session with initial state
            if 'session_tracker' in locals():
                session_tracker.update_interview_state(session_id, first_question_result["interview_state"])
            
        except Exception as interviewer_error:
            print(f"âŒ Autonomous Interviewer failed: {interviewer_error}")
            return {"error": f"Failed to generate opening statement: {interviewer_error}"}, 500
        
        # Create and Save the History with AI reasoning
        interview_history = [
            {
                "question": opening_statement,
                "answer": None,
                "timestamp": datetime.utcnow().isoformat(),
                "question_type": "opening",
                "ai_reasoning": first_question_result.get("chain_of_thought", []),
                "interview_state": first_question_result.get("interview_state", {}),
                "response_latency_ms": first_question_result.get("latency_ms", 0)
            }
        ]
        
        # Save history to Redis
        try:
            redis_url = os.environ.get('REDIS_URL')
            if not redis_url:
                raise ValueError("REDIS_URL environment variable is required")
            redis_client = redis.from_url(redis_url)
            
            history_json = json.dumps(interview_history)
            redis_client.set(f"history:{session_id}", history_json, ex=3600)  # Expire in 1 hour
            print(f"âœ… Interview history saved to Redis")
            
        except Exception as redis_error:
            print(f"âŒ Failed to save history to Redis: {redis_error}")
            return {"error": f"Failed to save interview history: {str(redis_error)}"}, 500
        
        # Return the Response with interview plan
        response_data = {
            "session_id": session_id,
            "opening_statement": opening_statement,
            "status": "started",
            "role": request.role,
            "seniority": request.seniority,
            "skill": request.skills[0] if request.skills else "General",
            "estimated_duration_minutes": 45,  # Default duration
            "message": "Interview started successfully with pre-interview planning",
            "interview_plan": {
                "archetype": interview_plan["selected_archetype"],
                "objective": interview_plan.get("evaluation_criteria", {}).get("seniority_adjustments", {}).get(request.seniority.lower(), "Standard evaluation"),
                "evaluation_dimensions": list(interview_plan["signal_map"].keys())
            }
        }
        
        print(f"ðŸŽ¯ Interview session {session_id} started successfully!")
        return response_data
        
    except Exception as e:
        print(f"âŒ Unexpected error in start_interview: {e}")
        return {"error": f"Interview start failed: {str(e)}"}, 500

@app.post("/api/submit-answer")
async def submit_answer(request: SubmitAnswerRequest):
    """
    Receives an answer from the user and processes it using the autonomous interviewer.
    """
    if not request.session_id or not request.answer:
        raise HTTPException(status_code=400, detail="session_id and answer are required.")
    
    try:
        print(f"ðŸŽ¯ Processing answer for session {request.session_id}")
        
        # Get Redis connection
        redis_url = os.environ.get('REDIS_URL')
        if not redis_url:
            raise ValueError("REDIS_URL environment variable is required")
        redis_client = redis.from_url(redis_url, decode_responses=False)
        
        # Fetch the Current State from Redis
        print("ðŸ“¥ Fetching current state from Redis...")
        
        # Get the conversation history
        history_json = redis_client.get(f"history:{request.session_id}")
        if not history_json:
            raise HTTPException(status_code=404, detail="Conversation history not found. Session may have expired.")
        
        conversation_history = json.loads(history_json.decode('utf-8') if isinstance(history_json, bytes) else history_json)
        print(f"âœ… Retrieved conversation history with {len(conversation_history)} turns")
        
        # Update the Conversation History
        print("ðŸ“ Updating conversation history with user's answer...")
        
        if not conversation_history:
            raise HTTPException(status_code=400, detail="Conversation history is empty")
        
        # Update the last turn's answer
        last_turn = conversation_history[-1]
        last_turn["answer"] = request.answer
        print(f"âœ… Updated last turn with user's answer: {request.answer[:50]}...")
        
        # Also add user's answer to session tracker
        try:
            session_tracker = SessionTracker()
            session_tracker.add_conversation_turn(request.session_id, "user", request.answer)
            print(f"âœ… Added user's answer to session tracker")
        except Exception as e:
            print(f"âš ï¸  Warning: Failed to add user's answer to session tracker: {e}")
        
        # Use Autonomous Interviewer for Response Generation
        print("ðŸŽ­ Using Autonomous Interviewer system for response generation...")
        
        try:
            autonomous_interviewer = AutonomousInterviewer()
            session_tracker = SessionTracker()
            
            # Get current session state
            session_data = session_tracker.get_session(request.session_id)
            if not session_data:
                raise Exception("Session not found or expired")
            
            # Create the conversation history for the AI with the user's answer included
            ai_conversation_history = []
            for turn in conversation_history:
                if turn.get("question"):
                    ai_conversation_history.append({
                        "role": "interviewer",
                        "content": turn["question"]
                    })
                if turn.get("answer"):  # Make sure we include user answers
                    ai_conversation_history.append({
                        "role": "user", 
                        "content": turn["answer"]
                    })
            
            print(f"ðŸ” AI Conversation History prepared: {len(ai_conversation_history)} turns")
            for i, turn in enumerate(ai_conversation_history):
                print(f"  Turn {i+1}: {turn['role']} - {turn['content'][:50]}...")
            
            print(f"ðŸ” Calling autonomous_interviewer.conduct_interview_turn with:")
            print(f"  - role: {session_data['role']}")
            print(f"  - seniority: {session_data['seniority']}")
            print(f"  - skill: {session_data['skill']}")
            print(f"  - interview_stage: {session_data['current_stage']}")
            print(f"  - conversation_history: {len(ai_conversation_history)} turns")
            print(f"  - session_context: {session_tracker.get_session_context(request.session_id)}")
            
            # Get the interview plan from the session context
            interview_plan = session_tracker.get_session_context(request.session_id).get("interview_plan", {})
            
            # Process the user response using enhanced autonomous interviewer with signal tracking
            interviewer_result = autonomous_interviewer.conduct_interview_turn(
                role=session_data["role"],
                seniority=session_data["seniority"],
                skill=session_data["skill"],
                interview_stage=session_data["current_stage"],
                conversation_history=ai_conversation_history,
                session_context=session_tracker.get_session_context(request.session_id),
                interview_plan=interview_plan
            )
            
            if not interviewer_result.get("response_text"):
                raise Exception("Autonomous Interviewer failed to generate response")
            
            new_ai_question = interviewer_result["response_text"]
            current_stage = interviewer_result["interview_state"]["current_stage"]
            skill_progress = interviewer_result["interview_state"]["skill_progress"]
            next_focus = interviewer_result["interview_state"]["next_focus"]
            
            print(f"âœ… Autonomous Interviewer generated response")
            print(f"ðŸ” FULL INTERVIEWER RESULT:")
            print(f"  - Chain of Thought: {interviewer_result.get('chain_of_thought', [])}")
            print(f"  - Response Text: {interviewer_result.get('response_text', 'No response text')}")
            print(f"  - Interview State: {interviewer_result.get('interview_state', {})}")
            print(f"ðŸ“Š Current stage: {current_stage}")
            print(f"ðŸŽ¯ Skill progress: {skill_progress}")
            print(f"ðŸŽ¯ Next focus: {next_focus}")
            
            # Update session state
            session_tracker.update_interview_state(request.session_id, interviewer_result["interview_state"])
            
            # Add the new question to conversation history
            session_tracker.add_conversation_turn(request.session_id, "interviewer", new_ai_question)
            
            # Check if interview should end (based on skill progress or other criteria)
            if skill_progress in ["expert", "advanced"] and len(conversation_history) > 10:
                print("ðŸŽ‰ Interview completed by Autonomous Interviewer!")
                # Mark session as completed
                session_tracker.update_session(request.session_id, {"status": "completed"})
            
        except Exception as interviewer_error:
            print(f"âŒ Autonomous Interviewer failed: {interviewer_error}")
            raise interviewer_error
        
        # Add the new question to conversation history with AI reasoning
        conversation_history.append({
            "question": new_ai_question,
            "answer": None,
            "timestamp": datetime.utcnow().isoformat(),
            "question_type": "follow_up",
            "ai_reasoning": interviewer_result.get("chain_of_thought", []),
            "interview_state": interviewer_result.get("interview_state", {}),
            "response_latency_ms": interviewer_result.get("latency_ms", 0)
        })
        
        # Save updated history to Redis
        try:
            history_json = json.dumps(conversation_history)
            redis_client.set(f"history:{request.session_id}", history_json, ex=3600)
            print(f"âœ… Updated conversation history saved to Redis")
        except Exception as redis_error:
            print(f"âš ï¸  Warning: Failed to save updated history: {redis_error}")
        
        # Return the new question
        result = {
            "success": True,
            "message": "Answer processed successfully",
            "next_question": new_ai_question,
            "session_id": request.session_id,
            "timestamp": datetime.now().isoformat(),
            "architecture": "autonomous_interviewer",
            "current_stage": current_stage if 'current_stage' in locals() else "unknown",
            "skill_progress": skill_progress if 'skill_progress' in locals() else "unknown"
        }
        
        print(f"ðŸŽ‰ Answer processing completed successfully for session {request.session_id}")
        return result
        
    except Exception as e:
        print(f"âŒ Unexpected error in submit_answer: {e}")
        return {"error": f"Answer processing failed: {str(e)}"}, 500

@app.post("/api/interview/{session_id}/complete")
async def complete_interview(session_id: str):
    """
    Completes an interview session and provides comprehensive evaluation.
    """
    try:
        print(f"ðŸ Completing interview session {session_id}")
        
        # Retrieve conversation history from Redis
        try:
            redis_url = os.environ.get('REDIS_URL')
            if not redis_url:
                raise ValueError("REDIS_URL environment variable is required")
            redis_client = redis.from_url(redis_url)
            
            history_json = redis_client.get(f"history:{session_id}")
            if not history_json:
                return {"error": "Interview history not found. Session may have expired."}, 404
            
            conversation_history = json.loads(history_json.decode('utf-8'))
            
        except Exception as redis_error:
            print(f"âŒ Failed to retrieve data from Redis: {redis_error}")
            return {"error": f"Failed to retrieve interview data: {str(redis_error)}"}, 500
        
        # Get session data
        try:
            session_tracker = SessionTracker()
            session_data = session_tracker.get_session(session_id)
            
            if not session_data:
                return {"error": "Session not found"}, 404
                
        except Exception as session_error:
            print(f"âŒ Failed to get session data: {session_error}")
            return {"error": f"Failed to get session data: {str(session_error)}"}, 500
        
        # Extract Q&A pairs for evaluation
        qa_pairs = []
        for i, turn in enumerate(conversation_history):
            if turn.get("question") and turn.get("answer"):
                qa_pairs.append({
                    "question": turn["question"],
                    "answer": turn["answer"]
                })
        
        if not qa_pairs:
            return {"error": "No questions and answers found for evaluation"}, 400
        
        print(f"ðŸ“Š Found {len(qa_pairs)} Q&A pairs for evaluation")
        
        # Evaluate each answer using the evaluation agent
        evaluations = []
        skills_to_assess = session_data.get("skills", ["Problem Solving", "Communication", "Technical Knowledge"])
        
        for qa in qa_pairs:
            print(f"ðŸ” Evaluating Q: {qa['question'][:50]}...")
            evaluation = evaluate_answer(qa["answer"], qa["question"], skills_to_assess)
            evaluations.append({
                "question": qa["question"],
                "answer": qa["answer"],
                "evaluation": evaluation
            })
        
        # Calculate overall scores
        all_scores = []
        for eval_data in evaluations:
            if "evaluation" in eval_data and "scores" in eval_data["evaluation"]:
                scores = eval_data["evaluation"]["scores"]
                for skill, score_data in scores.items():
                    if isinstance(score_data, dict) and "score" in score_data:
                        all_scores.append(score_data["score"])
        
        overall_score = round(sum(all_scores) / len(all_scores), 1) if all_scores else 0
        
        # Generate comprehensive feedback
        overall_summary = f"Interview completed with {len(qa_pairs)} questions. Overall performance score: {overall_score}/5"
        
        # Prepare detailed feedback structure
        feedback_data = {
            "overall_score": overall_score,
            "overall_summary": overall_summary,
            "questions_evaluated": len(qa_pairs),
            "skills_assessed": skills_to_assess,
            "detailed_evaluations": evaluations,
            "session_id": session_id,
            "role": session_data.get("role"),
            "seniority": session_data.get("seniority"),
            "completed_at": datetime.now().isoformat()
        }
        
        # Mark session as completed
        session_tracker.update_session(session_id, {"status": "completed"})
        
        # Persist the complete interview data to the database
        try:
            # Prepare conversation history with complete data
            complete_conversation_history = []
            for turn in conversation_history:
                turn_data = {
                    "question": turn.get("question"),
                    "answer": turn.get("answer"),
                    "timestamp": turn.get("timestamp"),
                    "question_type": turn.get("question_type", "follow_up")
                }
                complete_conversation_history.append(turn_data)
            
            # Prepare final state data
            final_state = {
                "final_stage": session_data.get("current_stage", "unknown"),
                "final_skill_progress": session_data.get("skill_progress", "unknown"),
                "total_response_time_ms": 0,  # Could be calculated from timestamps
                "average_score": overall_score,
                "completion_time": datetime.now().isoformat(),
                "ai_reasoning": []  # Could be enhanced to capture AI reasoning
            }
            
            # Persist to database
            persist_complete_interview(
                session_id=session_id,
                session_data=session_data,
                conversation_history=complete_conversation_history,
                evaluations=evaluations,
                final_state=final_state
            )
            print(f"âœ… Interview data persisted to database for session {session_id}")
        except Exception as db_error:
            print(f"âŒ Failed to persist interview data to database: {db_error}")
            # Continue with returning feedback data even if persistence fails
        
        print(f"âœ… Interview completion and evaluation finished for session {session_id}")
        return {"success": True, "data": feedback_data}
        
    except Exception as e:
        print(f"âŒ Unexpected error in complete_interview: {e}")
        return {"error": f"Interview completion failed: {str(e)}"}, 500

@app.get("/api/interview/{session_id}/status")
async def get_interview_status(session_id: str):
    """
    Gets the current status of an interview session.
    """
    try:
        print(f"ðŸ“Š Getting status for interview session {session_id}")
        
        # Retrieve data from Redis
        try:
            redis_url = os.environ.get('REDIS_URL')
            if not redis_url:
                raise ValueError("REDIS_URL environment variable is required")
            redis_client = redis.from_url(redis_url)
            
            # Get the history
            history_json = redis_client.get(f"history:{session_id}")
            if not history_json:
                return {"error": "Interview history not found. Session may have expired."}, 404
            
            interview_history = json.loads(history_json.decode('utf-8'))
            
        except Exception as redis_error:
            print(f"âŒ Failed to retrieve data from Redis: {redis_error}")
            return {"error": f"Failed to retrieve interview data: {str(redis_error)}"}, 500
        
        # Get session state from session tracker
        try:
            session_tracker = SessionTracker()
            session_data = session_tracker.get_session(session_id)
            
            if not session_data:
                return {"error": "Session not found"}, 404
                
        except Exception as session_error:
            print(f"âŒ Failed to get session data: {session_error}")
            return {"error": f"Failed to get session data: {str(session_error)}"}, 500
        
        # Calculate progress
        questions_asked = len([qa for qa in interview_history if qa.get("question")])
        questions_answered = len([qa for qa in interview_history if qa.get("answer") is not None])
        
        # Prepare response
        status_data = {
            "session_id": session_id,
            "role": session_data.get("role"),
            "seniority": session_data.get("seniority"),
            "skill": session_data.get("skill"),
            "questions_asked": questions_asked,
            "questions_answered": questions_answered,
            "status": session_data.get("status", "in_progress"),
            "current_stage": session_data.get("current_stage"),
            "skill_progress": session_data.get("skill_progress"),
            "start_time": session_data.get("start_time")
        }
        
        print(f"âœ… Interview status retrieved successfully for session {session_id}")
        return status_data
        
    except Exception as e:
        print(f"âŒ Unexpected error in get_interview_status: {e}")
        return {"error": f"Failed to get interview status: {str(e)}"}, 500

# --- Prompt Evaluation Endpoints ---
from agents.prompt_evaluator import prompt_evaluator

@app.get("/api/prompt-evaluation/overview")
async def get_prompt_evaluation_overview(hours: int = 24):
    """Get overview of prompt evaluation metrics"""
    try:
        analysis = prompt_evaluator.get_prompt_effectiveness_analysis(hours)
        return analysis
    except Exception as e:
        print(f"âŒ Error in get_prompt_evaluation_overview: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get overview: {str(e)}")

@app.get("/api/prompt-evaluation/components")
async def get_prompt_evaluation_components():
    """Get list of available components"""
    try:
        # Get unique components from database
        executions = prompt_evaluator.get_executions(limit=1000)
        components = list(set(execution.component for execution in executions))
        return components
    except Exception as e:
        print(f"âŒ Error in get_prompt_evaluation_components: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get components: {str(e)}")

@app.get("/api/prompt-evaluation/component/{component}")
async def get_component_analysis(component: str, hours: int = 24):
    """Get detailed analysis for a specific component"""
    try:
        analysis = prompt_evaluator.analyze_component_performance(component, hours)
        if not analysis:
            raise HTTPException(status_code=404, detail=f"Component {component} not found")
        return analysis
    except Exception as e:
        print(f"âŒ Error in get_component_analysis: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to analyze component: {str(e)}")

@app.get("/api/prompt-evaluation/executions")
async def get_prompt_executions(limit: int = 100, success_only: bool = None):
    """Get recent prompt executions"""
    try:
        executions = prompt_evaluator.get_executions(limit=limit, success_only=success_only)
        # Convert to serializable format
        serializable_executions = []
        for execution in executions:
            serializable_executions.append({
                "execution_id": execution.execution_id,
                "timestamp": execution.timestamp,
                "component": execution.component,
                "method": execution.method,
                "prompt_type": execution.prompt_type,
                "input_data": execution.input_data,
                "prompt_text": execution.prompt_text,
                "output_data": execution.output_data,
                "response_text": execution.response_text,
                "latency_ms": execution.latency_ms,
                "token_count": execution.token_count,
                "success": execution.success,
                "error_message": execution.error_message,
                "session_id": execution.session_id,
                "user_id": execution.user_id,
                "role": execution.role,
                "seniority": execution.seniority,
                "skill": execution.skill
            })
        return serializable_executions
    except Exception as e:
        print(f"âŒ Error in get_prompt_executions: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get executions: {str(e)}")

@app.get("/api/prompt-evaluation/session/{session_id}")
async def get_session_analysis(session_id: str):
    """Get analysis for a specific interview session"""
    try:
        analysis = prompt_evaluator.get_session_analysis(session_id)
        return analysis
    except Exception as e:
        print(f"âŒ Error in get_session_analysis: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to analyze session: {str(e)}")

@app.get("/api/prompt-evaluation/export")
async def export_prompt_executions(hours: int = 24):
    """Export prompt executions to JSON"""
    try:
        success = prompt_evaluator.export_executions_to_json("prompt_executions_export.json", hours)
        if not success:
            raise HTTPException(status_code=500, detail="Failed to export executions")
        
        # Return the file
        from fastapi.responses import FileResponse
        return FileResponse("prompt_executions_export.json", media_type="application/json")
        
    except Exception as e:
        print(f"âŒ Error in export_prompt_executions: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to export: {str(e)}")

@app.post("/api/prompt-evaluation/clear")
async def clear_prompt_evaluation_database():
    """Clear all prompt evaluation data (for debugging)"""
    try:
        import sqlite3
        import os
        
        db_path = "prompt_evaluations.db"
        if os.path.exists(db_path):
            os.remove(db_path)
            # Reinitialize the database
            prompt_evaluator._init_database()
        
        return {"message": "Database cleared successfully"}
        
    except Exception as e:
        print(f"âŒ Error in clear_prompt_evaluation_database: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to clear database: {str(e)}")

@app.post("/api/prompt-evaluation/test")
async def test_prompt_capture():
    """Test prompt capture functionality"""
    try:
        execution_id = prompt_evaluator.capture_execution(
            component="test",
            method="test_method",
            prompt_type="test",
            input_data={"test": "data"},
            prompt_text="Test prompt",
            output_data={"test": "response"},
            response_text="Test response",
            latency_ms=100.0,
            success=True
        )
        return {"execution_id": execution_id, "message": "Test prompt captured successfully"}
        
    except Exception as e:
        print(f"âŒ Error in test_prompt_capture: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to test prompt capture: {str(e)}")

@app.post("/api/prompt-evaluation/generate-test-data")
async def generate_test_prompt_data():
    """Generate sample test data for demonstration"""
    try:
        import random
        import time
        
        # Generate some sample executions
        components = ["autonomous_interviewer", "evaluation", "pre_interview_planner"]
        methods = ["conduct_interview_turn", "evaluate_answer", "create_interview_plan"]
        prompt_types = ["interview_question", "evaluation", "planning"]
        roles = ["Product Manager", "Software Engineer", "Data Analyst"]
        seniorities = ["Junior", "Mid", "Senior", "Manager"]
        skills = ["System Design", "Problem Solving", "Communication"]
        
        count = 0
        for i in range(20):
            try:
                execution_id = prompt_evaluator.capture_execution(
                    component=random.choice(components),
                    method=random.choice(methods),
                    prompt_type=random.choice(prompt_types),
                    input_data={
                        "role": random.choice(roles),
                        "seniority": random.choice(seniorities),
                        "skill": random.choice(skills),
                        "test_data": True
                    },
                    prompt_text=f"Test prompt {i+1}",
                    output_data={"test": "response", "iteration": i+1},
                    response_text=f"Test response {i+1}",
                    latency_ms=random.uniform(50, 500),
                    success=random.random() > 0.1,  # 90% success rate
                    error_message="Test error" if random.random() <= 0.1 else None,
                    session_id=f"test_session_{random.randint(1, 5)}",
                    role=random.choice(roles),
                    seniority=random.choice(seniorities),
                    skill=random.choice(skills)
                )
                count += 1
                time.sleep(0.01)  # Small delay to ensure unique timestamps
                
            except Exception as e:
                print(f"Failed to generate test execution {i+1}: {e}")
                continue
        
        return {"count": count, "message": f"Generated {count} test prompt executions"}
        
    except Exception as e:
        print(f"âŒ Error in generate_test_prompt_data: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to generate test data: {str(e)}")

# --- Startup Event Handler ---
@app.on_event("startup")
async def startup_event():
    """Run startup checks when the FastAPI app starts"""
    print("ðŸš€ PrepAI Autonomous Interviewer Backend Starting Up...")
    
    # Run comprehensive startup checks including database migration verification
    try:
        from startup import run_startup_checks
        success = run_startup_checks()
        if success:
            print("âœ… All startup checks passed! Service is ready to serve requests!")
        else:
            print("âš ï¸  Some startup checks failed, but service will continue...")
    except Exception as e:
        print(f"âš ï¸  Startup checks failed with error: {e}")
        print("Service will continue, but some features may not work properly...")
    
    print("âœ… Service is ready to serve requests!")


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
